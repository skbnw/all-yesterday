headline,mainEntityOfPage,image,datePublished,dateModified,author,media_en,media_jp,str_count,body,images,external_links
DeepSeek「後」の世界で起きること、はびこる「米国vs中国」論争が見当違いすぎるワケ（ビジネス＋IT）,https://news.yahoo.co.jp/articles/3d6f77e2c7124783f41a8990b6885402e3d79dbb,https://newsatcl-pctr.c.yimg.jp/t/amd-img/20250207-00157509-biz_plus-000-1-view.jpg?exp=10800,2025-02-07T07:10:05+09:00,2025-02-07T07:10:05+09:00,ビジネス＋IT,biz_plus,ビジネス＋IT,5192,"DeepSeekショックの「本質」とは（Photo/Erlin Diah/Shutterstock.com）
中国のAIスタートアップDeepSeekが公開したAIモデル「DeepSeek-R1」が、OpenAIのGPTと比べて学習コスト1/10で、ほぼ同等の性能を達成したことから、主要AI各社の株価が暴落し世間を賑わせている。ただし、この状況を「米国AI vs 中国AI」と語るのは見当違いだ。このDeepSeekショックの“本質”を捉えると、これから起きることが見えてくる。
【詳細な図や写真】1月24日の終値を基準にした株価の下落率は、GAFAMは大きくない。3%から4%の小幅下落にとどまり、その後、1月24日の終値を超えて上昇している。下落率が大きかったのが、エヌビディア、ブロードコム、電力企業各社だった（データ出典：各社株価情報、ロゴ出典：Shutterstock.com、各社HP）
DeepSeekショック…本当に打撃を受けたのは誰か？
DeepSeekショックに世界が揺れている。中国の「深度探索」（DeepSeek）が1月20日に公開したDeepSeek-R1が、OpenAIのGPT-o1に匹敵する性能でありながら、そのトレーニングコストが1/10以下であることが判明すると、週明けの米国関連企業の株価が急落したからだ。

　「米国AIの優位性が揺らぐ」と言う人もいるが、米国のビッグテック「GAFAM」の株価はさほど落ちていない（OpenAIは未上場）。DeepSeekショックが起こる前の1月24日の終値を基準にして、その後の最安値の下落率を調べてみると、GAFAM各社の下落率は3％から4％にすぎず、アップルは下落すらしていない。さらにその後、各社ともDeepSeekショック前の株価を上回っている。

　一方、ショックを受けたのは、AI開発に欠かせない並列計算を行うGPUを製造しているエヌビディアとデータセンターなどにネットワーク機器を提供しているブロードコムだ。下落率は17%にもなる。さらに打撃を受けたのが、電力ベンチャーだった。主要電力ベンチャーの株価は20%から30%も暴落をしており、各社とも1月24日の終値に戻しきれていない。


　つまり、投資家たちは「米国AIの優位性が揺らぐ」のではなく「GPUやデータセンター、電力のようなAI基盤の需要が弱まる」と見ている。
「米国AI vs 中国AI」がミスリードなワケ
AI開発の世界では「スケール法則」がある。それは規模を大きくすることで大きなブレイクスルーが起きたという成功体験によるものだ。

　2006年に提唱されたディープラーニングも、「AIの父」と称されるジェフリー・ヒントン氏の「ニューラルネットワークを多層化してみたらどうなるのか」という発想から生まれた。OpenAIのChatGPTも「TransformerベースのAIモデルのパラメーター数を増やしてみたらどうなるのか」という発想で生まれた。つまり、莫大な資金を投入し、大規模なAI基盤を整えることで大きなブレイクスルーが起きる。

　その頂点とも言えるのが、ドナルド・トランプ米大統領が発表したスターゲート計画だ。4年間で5,000億ドルを投資してAI基盤を整備するという計画で、皮肉にもその発表と前後して、DeepSeek-R1が公開された。今起きているのは、「米国AI vs 中国AI」ではなく「大規模投資AI vs 適正規模投資AI」なのだ。

　DeeSeekが2024年12月に公開した技術報告書「DeepSeek-V3 Technical Report」によると、DeepSeek-V3は、わずか2048枚のエヌビディアの「H800」でトレーニングされ、そのコストは1時間2ドルで計算すると557.6万ドル（約8.7億円）になる。OpenAIのGPT-4oのトレーニングコストは明らかにされていないが、1億ドル前後だと推定されている。

　つまり、DeepSeekは1/10以下のコストで開発されたことになる。それでいて、性能はGPT-4oと遜色がない。

　さらに重要なのは、DeepSeekはオープンソース（ソースコードが公開され、誰でも自由に使用、改変ができる）であり、同じくオープンソース戦略を取るメタのLlama（ラマ）、アリババの通義（Qwen）と比較すると、確実に性能が上回っている。この「オープンソースAIの中で最強」も大きな意味を持っている。
なぜ、AIエンジニア「たった70人」で開発できたのか？
では、DeepSeekはなぜ低コストで開発できたのか。

　答えは1つではなく、すべてにおいて最適化手法を試していることにある。

　その概要を解説した「DeepSeek-V3 Technical Report」は、最適化手法の見本市のようになっている。多数の最適化手法に挑戦し、冗長計算の80%を削減することに成功したという。独創というよりは改善を突き詰めていった結果のようだ。

　これを見た、とある日本のAIエンジニアは、「想像以上に泥臭いことをやっているようだ。（高額報酬になりがちな）米国のAIエンジニアにはできないだろう」と舌を巻く。

　この技術文書だけを見ても、AIエンジニア70人という小さな企業で、アイデアが次から次へと出てきて、激論を交わしながら手を動かすという熱気の中で開発が進んだことがうかがえる。

　その中で重要な技術が、MoE（Mixture of Experts）アーキテクチャーだ。これはグーグルが考案したものでグーグルの生成AI「Gemini」などにも使われているが、DeepSeekはこれを洗練させ、最適化した。

　通常のAIモデルは、何でも知っている博識のモデルが1つで、すべてのタスクに対応する。ところが、MoEは、小さな専門家モデルから構成されている。DeepSeek-V3の場合、タスクの割り振り担当1、各種専門家256の合計257モデルの集合体になっている。

　DeepSeekの研究によると、同パラメーター数（脳の神経細胞の数に相当）の単体モデル方式に比べて、トレーニングコストは40%減少し、推論スピードは80％向上するという。また、博識モデルが回答を生成するのではなく、得意分野を持つ専門家モデルが回答を生成するため、モデルは小さくても精度の高い回答を出すことができる。

　このようなモデルの学習にもさまざまな工夫がされている。DeepSeekは蒸留という手法を効果的に使った。蒸留とは、大きなAIモデルの入力と出力を、小さなモデルに学習させる手法だ。言わば、見て学ばせる、まねをさせるという感覚に近く、音声認識モデルなどでよく使われる手法だ。精度の高い大きなモデルを開発し、これを蒸留手法で小さなモデルに学ばせ、レコーダーなどの小さなデバイスに搭載をする。精度はある程度落ちるものの、AIモデルの軽量化が可能になる。

　AIモデルは、事前学習で質の高い学習素材を学ばせると、その後の学習効果が高くなる。そのため、DeepSeekは、事前学習でこの蒸留手法を使った。Llama、Qwenの蒸留を教材として学習を始めた。この2つは、オープンソースであるため、蒸留に利用しても法的な問題は生じない。

　ところが、OpenAIとマイクロソフトは、GPTもこの蒸留に利用されたとして調査を進めている。この主張が真実ならば、GPTはクローズドモデルであるため、利用規約違反などの問題が生じる。

　一方で、GPTはネット上のコンテンツを権利者の断りなく学習素材として利用しており、作家やメディアから訴訟を起こされている。OpenAIは自分が利用するときは「フェアユース」だと主張し、自分たちの成果物は他人に利用させないのは矛盾ではないかという論争も起きている。

　AIエンジニアの中には、AIモデルはクローズドにせず、すべてオープンソースにすべきだという考え方を持っている人も多く、DeepSeekショックは「クローズドAI vs オープンソースAI」という対立軸も生み出している。
DeepSeek「後」に起きること、何がどう変わるのか？
このMoEアーキテクチャーは、回答を生成する時も有利になる。DeepSeek-V3は6710億ものパラメーターを持っているが、回答は小さな専門家モデルが生成するために平均して370億程度のパラメーターしか活性化されない。全体の5.5%程度のリソースで生成するため、処理速度も早くなる。

　この軽さにより、SNSでは、すでにMac Studio（Macの最上位機種）やMac miniを数台連結したシステムでDeepSeekの動作に成功した例が投稿されている。

　DeepSeekはオープンソースであるため、誰でも自由にダウンロードすることができ、自分の環境で動作させることが可能だ。オープンソース配布プラットフォーム「Hugging Face」では、すでにDeepSeek-R1が100万回以上もダウンロードされている。


　これにより、企業への本格導入が進む可能性がある。これまでは、生成AIの利用に及び腰になっている企業も多かった。

　多くのAIモデルは、パラメーター数が多すぎて、もはや普通のPCでは動かすことができない。そのため、WebやAPI経由で情報を送信し、サーバで処理してもらい、その結果を返してもらうという使い方になっている。この方式では、セキュリティ評価ができないために企業内部の情報を送信することができず、限定的な使い方にならざるを得なかったのだ。

　しかし、高性能PCで動作するとなると、社内ネットワークの中に設置して、外部送信をすることなく利用できるようになる。社内ネットワークのセキュリティさえ確保されていれば、業務情報や顧客の個人情報もAIに処理してもらえるようになる。企業でのAIの本格導入が一気に進む可能性がある。
計画が狂ったメタやアリババ、今後の競争の「軸」は
そもそも、メタとアリババは、オープンソースにすることで、企業や組織に導入が進むことを狙っていた。導入で利益を生むことはできなくても、プラットフォームとしてアプリケーションを販売したり、追加開発、コンサルなどの業務が派生する。この点で、メタとアリババは戦略の見直しを迫られることになる。米メディアThe Informationは、メタはすでに「War Room」（緊急対策室）を設置したと報じている。

　DeepSeekは大量の論文を発表しており、その技術についてはガラス張りになっている。米国AI各社はその分析を行い、DeepSeekの最適化手法をさらに洗練させて取り込もうとしているはずで、「米国AIの優位性が揺らぐ」とは考えづらい。米国AI各社の動きは早い。すでにマイクロソフトのAzure AI、エヌビディアのNIM、アマゾンのAWSでは、利用者に向けてDeepSeekの提供も始めている。

　起きているのは「米国AI vs 中国AI」ではなく、「大規模投資AI vs 最適投資AI」「クローズドAI vs オープンソースAI」という対立だ。

　米中以外の投資環境が整ってなく、AI基盤が充実していない国にも大きなチャンスが巡ってきている。常識的な投資額でAI開発が可能であることが証明され、欧州や北欧、中東やインドといった国から、世界に通用するAIモデルが登場しても不思議ではなくなっている。もちろん、日本もその候補の1つだ。

　また、DeepSeek-R1で使われているGRPO（Group Relative Policy Optimization）強化学習という技術も注目に値をする。

　従来のAIモデルは、ネットコンテンツを学習素材とするため、既知のことしか回答することができなかった。しかし、DeepSeek-R1やGPT-o1などは推論機能を備えているため、未知のことについても考えて回答をすることができる。

　この機能を備えるには、強化学習が必要になる。AIモデルに試行錯誤させ、それに点数をつけ、報酬を与えたり罰を与えたりして学習をしていく。GRPOは、この強化学習の自動化につながる技術だ。まだ、完全自動化は達成していないが、近い将来、AIは人の手を介さずに自分で学習を進めて賢くなっていくことができるようになる。

　これがシンギュラリティ（技術的特異点）の始まりとなる。人類はそこまで進んできている。
執筆：ITジャーナリスト 牧野 武文",[],[]
三菱UFJ銀行がIOWNに参画する深すぎる理由、金融サービスの何が変わるか？（ビジネス＋IT）,https://news.yahoo.co.jp/articles/1d4e0cacc9f6155c6f4bec92ff1b3953bea37063,https://newsatcl-pctr.c.yimg.jp/t/amd-img/20250207-00157347-biz_plus-000-1-view.jpg?exp=10800,2025-02-07T06:30:05+09:00,2025-02-07T06:30:05+09:00,ビジネス＋IT,biz_plus,ビジネス＋IT,4305,"IOWNと金融（Photo/Shutterstock.com）
本稿では、NTTグループが注力するIOWNについて取り上げ、金融業界への影響を中心に議論を展開する。筆者はNTTグループ企業に所属しているが、IOWNの直接的な業務には関与しておらず、公に公開されている情報を基に執筆している。公開情報をもとに、なぜ三菱UFJ銀行がIOWNのプロジェクトに参画する理由や今後の展望について、筆者の視点から考察を行う。
【詳細な図や写真】IOWN構想（出典：NTTデータ報道発表）
そもそもIOWNとは何か？
IOWNとはInnovative Optical and Wireless Networkの略で、NTTが2019年から提唱している構想であり、光技術を中心とした革新的技術によって低消費電力・低遅延・大容量通信を実現する、第6世代移動通信（6G）時代を見据えたネットワーク・情報処理基盤を指している。

　IOWN構想は特定の技術を指すものではなく、さまざまな先端技術要素を組み合わせて便利な世界を目指そうとするための技術群のことを指すため、内包する技術要素はかなりの幅がある。


　このIOWN構想が打ち出された背景としては、社会の情報化がますます進展していく中で我々の生活は大きく変化し、またそれに多様な価値観が出てくると考えられる中、データ利用の増加に伴う既存の情報インフラや消費電力が限界を迎えつつあるということが挙げられる。

　生成AIに代表されるように、非常に多くのデータ量が飛び交い、また非常に多くの電力を使われるという状況は今後の社会の持続的成長にとっては大きな懸念事項である。

　こういった課題に解決すべくNTTの強みである光技術と従来の電子技術を融合させた「光電融合」技術を中心にIOWNは構想されているといっていいだろう。
IOWNを構成する主要技術
IOWNを構成する主要技術として、「オールフォトニクス・ネットワーク」「デジタルツインコンピューティング」「コグニティブ・ファウンデーション」が挙げられる。それぞれについて簡単に説明する。


・オールフォトニクス・ネットワーク（All Photonic Network：APN）
　ネットワーク回線だけでなく、中継装置、端末、チップの中の通信に光技術を適用することにより、これまでの電子ベースの技術では実現が困難であった「大容量化」「低遅延化」「低消費電力」の実現を目指している。要は光の適用範囲を極大化する中でこれまでの電子ベースの技術の限界を乗り越えようとしているいうことである。



・デジタルツインコンピューティング（Digital Twin Computing：DTC）
　現実世界を構成するモノ、ヒトなどをデジタル空間上にリアルに再現し、高度なシミュレーションを可能とする技術のことをデジタルツインコンピューティングと言うが、デジタルツインを用いることで、デジタル空間内で対象物に関する現状分析、将来予測、可能性のシミュレーションなどを行うことが可能となる。これにより社会課題の解決や革新的サービスの創出を目指している。



・コグニティブ・ファウンデーション（Cognitive Foundation：CF）
　ここまでの説明ででてきたような低消費電力・大容量・高品質のコミュニケーションや、大規模なモノ・ヒトの相互作用の再現・試行をシミュレーションするためには、さまざまなコンピューティングリソースを適切に選択・利用することが必要となる。コグニティブ・ファウンデーションは、さまざまな拠点に散在するデータを収集、処理、記憶、通信する手段を連携させて、サービスの構築や運用に必要なリソース調整する基盤である。
　以上のAPN、DTC、CFがIOWNの3つの主要な技術であるが、これに加えて最近はIOWN PETs（Privacy-Enhancing Technologies）と呼ぶ高セキュアで効率的な分散データ管理と秘匿処理を行う領域にも力を入れている。

　この領域はいわゆるデータスペースと言われている概念にも非常に近く、単に高速、大容量のデータ伝送だけではなく実際にデータを利活用するにあたってセキュアであることを支援している。
三菱UFJ銀行がIOWNに参画する深すぎる理由
ここまではIOWNの全体像について説明したが、ここからは具体的にどういった検討がなされているかを紹介したい。IOWNのサービスとしては2023年に、通信ネットワークの全区間で光波長を専有するAPNとして提供する「APN IOWN1.0」を開始しており、100Gbpsの回線で従来比200分の1の低レイテンシおよびゆらぎゼロを実現している。

　今後も新しいサービスが続くことが想定され、構想がよりリアリティを持った形で具現化していくことなるだろう。

　ただ、いかによいサービスであってもそれに資するユースケースがないことには宝の持ち腐れである。こういったこともあり「IOWN Global Forum」というコミュニケーションの未来を考える非営利団体では技術仕様の整理と並行してユースケースの検討にも取り組んでいる。

　この「IOWN Global Forum」に参加している金融機関である三菱UFJ銀行が2024年8月、ホワイトペーパー（Services Infrastructure for Financial Industry Use Case）を発行した。

　このホワイトペーパーの中で分散化したデータセンターの配置や遠隔地におけるデータセンターの即時同期によって、災害時のデータ保護や迅速な復旧を実現するための具体的な手順など非常に興味深い検証結果を公開している。


　こういった議論が現実のユースケースにつながってくることになると考えているが、一方で現時点での金融ビジネスでユースケースはまだない認識で、今後に期待するところである。
IOWNは金融の何を変えるか？
ここからはIOWN構想でうたわれていることが実現したとしてどういうことが金融ビジネスに起きるのかを少し考えてみたい。

　先ほどの三菱UFJ銀行のホワイトペーパーにもあるようにデータセンターとの接続やデータセンター間の接続が低レイテンシになることで、たとえば各データセンターの電源供給を各地の再生可能エネルギーの発電状況に応じて調整できるようになる。

　これにより、可能な限り再生可能エネルギーを利用する（化石燃料から発電した電力を極力使わない）というオペレーションも可能になるだろう。

　また、銀行などの営業店の人員の配置にも影響するかもしれない。現在でもテレビ会議、オンライン会議などのツールがあるので遠隔地での接客ということがまったくできていないというわけではない。ただ、ツールを経由した際の音声、画像の精細度にストレスがないというとそんなことはない。

　したがってこのあたりストレスがAPNの大容量、低遅延の通信によって解消し、ほぼ対面にちかい顧客体験が実現するのであれば、対面前提で営業店に配置していた要員を再配置することも可能になるのでないかと考える。

　また、これは金融に限った話ではないが、現状の生成AIブームを見ていてわかるようにAIのビジネス活用はより一層進んでいくことなるだろう。

　その際にAIをビジネスの武器にするのであればそのための学習データ等を大量にやり取りし、またAIに学習させるために大量のマシンパワーを使うことになる。

　こういったことを効率的にかつできるだけ環境にやさしく行う（＝電力をできるだけ使わない）ためにはIOWNに期待することも大きいだろう。
IOWNの先にある未来と金融
さて、ここまではIOWNの概要と金融ビジネスでの可能性について述べてきたが、最後にIOWNの先にある未来像についてコメントしたい。

　大容量かつ低遅延の通信が社会全体に普及するといったい何が起きるのだろうか？ という点を考えていくと、たとえば紛失すると困るものは極力持たないでよいのでは？ という世界観はありそうな気がしている。

　現在のスマートフォンには個人情報だけではなく電子マネーの残高などさまざまな情報が格納されている。したがってスマートフォンを紛失するといろいろな問題が発生するわけだがこれが大容量、低遅延の通信がある前提であれば、いわゆるシンクライアント端末としてスマートフォンを使うことが可能となり、大事な情報の本体をセキュアなクラウドで管理することになるだろう。

　また、スマートフォン本体に電子マネーなどの残高を保持しなくても、たとえばコンビニエンスストアの店頭のカメラで本人認証をしてその認証をもって決済を行う世界になるならキャッシュレスならぬウォレットレスということも視野に入ってくる。

　企業に目を転じると現在以上に企業活動にかかるデータは発生することになるが、そのデータを分析することでたとえばサプライチェーンの最適化に向けての取り組みが加速することになるだろう。

　IOWNの技術要素の中には量子コンピューターも含まれているが、量子コンピューターがこれまで計算できなかった領域に解をもたらす可能性があり、そういった意味でも企業活動ひいては社会全体の最適化に向けた動きに繋がっていくことなるだろう。


●参考リンク●
IOWN-PETs | NTT R&D Website
APN IOWN1.0の提供開始について | お知らせ・報道発表 | 企業情報 | NTT東日本
今、知っておくべき「データスペース」の現在地 ～国や企業の壁を超えたデータ連携で、未来はこう変わる～ | DATA INSIGHT | NTTデータ - NTT DATA
IOWN-PETs | NTT R&D Website
mufg_nttdata_use_case_iown_jpn.pdf
Use Cases - IOWN Global Forum - Innovative Optical and Wireless Network
IOWN Global Forumの最新動向 | NTT R&D Website

本稿に記載の見解は執筆者の個人的な見解であり、弊社を代表する意見ではありません。
執筆：NTTデータ 金融イノベーション本部ビジネスデザイン室 統括部長 山本 英生",[],[]
担当者が「燃え尽き症候群」……ガートナーが語る「長続きさせる」セキュリティ対策（ビジネス＋IT）,https://news.yahoo.co.jp/articles/d7c0c4b03d018ea419c782add18362bc20aa0622,https://newsatcl-pctr.c.yimg.jp/t/amd-img/20250207-00157450-biz_plus-000-1-view.jpg?exp=10800,2025-02-07T06:10:06+09:00,2025-02-07T06:10:06+09:00,ビジネス＋IT,biz_plus,ビジネス＋IT,3450,"ゼロトラストへの取り組みは「継続的な」可視化が求められる（出典：Gartner（2024年10月））
企業のセキュリティ対策は場当たり的な対応の連続になりがちで、セキュリティ担当者が「燃え尽き症候群」に陥りがちだ。その回避に向けてカギを握るのが、中・長期的な視点に基づく継続的な対応策の検討である。Gartner バイスプレジデント, アナリストの礒田優一氏が、セキュリティ・ガバナンスの重要性を解説するとともに、担当者が疲弊しないために中長期の視点で検討すべき「ゼロトラスト戦略」や「AIガバナンス」など、5つのアジェンダを紹介する。
【詳細な図や写真】日本におけるセキュリティの重要論点（出典：Gartner（2024年10月））
場当たり的なセキュリティ対応からの脱却
技術の急速な革新やサイバー攻撃の凶悪化、法制面の変更などを背景に、セキュリティ領域における課題は高度化・複雑化する一方だ。その中での対応は、得てして個々の課題に特化した、場当たり的なものになりがちだ。

　「無論、それらが重要なことは言うまでもありません。ただ、目前の課題に注力するだけでは疲弊し、担当者が燃え尽き症候群に陥りかねません」と注意を促すのは、Gartner バイスプレジデント, アナリストの礒田優一氏である。

　回避に向けた“処方箋”として礒田氏が推奨するのが、状況を俯瞰的に捉えた上での中・長期的な視点での対応策の検討である。そして、その出発点と位置付けるのが、自律的・継続的なセキュリティ改善に向けた、多様な変化が続く中での「セキュリティ・ガバナンスの確立」だ。


　礒田氏がそのために検討すべきテーマとして示したのが次の5つである。1つ目は「経営とのコミュニケーション」だ。
経営の「素朴な疑問」に応える重要性
一昔前、経営層の中には情報セキュリティを経営とは別次元の問題だと捉える人が少なくなかった。それも今では大きく改善し、「世界の取締役の84％がサイバーセキュリティをビジネスリスクとみなし、経営者自身が説明責任を含め法的責務を負うと考えるようになっています」（礒田氏）。


　ただ、セキュリティリーダーにとって厄介なのが、サイバー攻撃や新たな働き方、AIとその規制法など、リスクの多様化・複雑化を背景に、経営層へのセキュリティに関する適切な報告の難度が着実に増していることである。

　その中で礒田氏が必要性を強調するのが、最低でも年に1回以上の経営層への自社が抱えるリスクの説明と、その低減に向けた施策の伝達だ。

「その中でおすすめしているのが、『規制に対応できているか』『責任ある企業として最低限の対応ができているか』『どこまでやるのか、他社とくらべてどうか』など、“経営の素朴な疑問”に応える指標を用いた説明です。これなら関心を持ってもらいやすく、セキュリティリスクについて、それだけ深い経営との議論が可能です。マルウェアの更新状況や検知数などの従来からの報告では技術の問題と捉えられかねず、経営課題として認識してもらいにくい点に留意すべきです」（礒田氏）

　検討テーマの2つ目は、「各拠点のセキュリティ・ガバナンス」だ。

　現状を俯瞰して捉えれば、保護すべき対象はサプライチェーン全体にまで拡大する一方で、脅威は巧妙化・凶悪化する一方だ。「サイバー攻撃は構造的に攻撃側が圧倒的に有利で、現実的にすべての保護は極めて困難です」と礒田氏は説明する。
説明責任を果たすための「仕組み作り」が重要に
そこでセキュリティリーダーに求められているのが、「インシデント発生を前提に、日頃から十分な対策を講じてきたことを説明するための仕組みや仕掛けづくり」だ。その中で戦略や計画、ポリシーなどの「ガバナンス」、責任者や教育などの「体制」、脆弱性マネジメントやインシデント対応などの「プロセス」、ネットワークセキュリティなどの「技術」を固め、環境の変化を踏まえて継続的に見直していくべきだと礒田氏は提言する。

「100点を目指すのではなく、60点を目指す継続的かつ地道な取り組みです。しかし、万一の際の説明責任を果たすためにも、着実に実行する必要があります。セキュリティ対策は本社側が主導する“集約”と、現場に委任する“分散”の2つのアプローチがあり、どちらに軸足を置くべきかはケース・バイ・ケースとなりますが、中長期で徐々に集約するのが自然な流れでしょう」（礒田氏）

　検討テーマの3つ目は「ゼロトラスト戦略」である。

　礒田氏はゼロトラストについて「安易に信頼しないという考え方に基づく、継続的な可視化・検証の仕組みです」と説明する。そのために活用を見込めるツールは極めて多岐にわたる。

　その中で実施が抜けがちなものとして礒田氏が挙げるのが、サイバーセキュリティリスクを継続的に監視・管理する手法の「CTEM（Continuous Threat Exposure Management：継続的脅威エクスポージャー管理）」だ。いわゆる脆弱性管理の取り組みであり、短期と中期、長期の取り組みを通じ、脅威分析と対応を段階的に高度化、自動化する。

　もう1つの抜けがちなものが、場当たり的な対応による弊害としての「セキュリティ戦略の策定」だ。

「その場合には自分たちの立ち位置を確認した上で、短期、中期、長期で、ユーザーやアプリ、デバイス、データなどシステムの要素について課題を洗い出し、整理し、対応の方向性を検討します。米国防総省のゼロトラストに関するガイドラインが大いに参考になるはずです」（礒田氏）
「責任あるAI原則」のためにAIガバナンスを確立する
検討テーマの4つ目が、「人中心のセキュリティ」だ。

　ITの利活用がこれほど広がったことで、それらすべてのセキュリティ部門による保護は極めて困難となっている。ガートナーの調査でも64％の組織が「中央でのすべての管理は困難」だと回答している。そこでの対策の現実解となるのが「人中心のセキュリティ」だという。

「従来の対策は子供相手のように、細かなルールでユーザーを縛っていました。対して人中心のセキュリティでは、大人のようにユーザーに自由と責任を与えた上で、モニタリングでセキュリティを担保します」（礒田氏）

　人中心のセキュリティを採用する企業はグローバルで着実に広がっているという。たとえば、とある大手メーカーは技術に精通したビジネステクノロジストを現場に配置。

「リスクの低いプロジェクトを現場主導で実施できる体制を整えることで、イノベーションを加速させるなどの成果を上げています」（礒田氏）

　検討テーマの5つ目が、「AIガバナンス」である。

　技術は良くも悪くも活用でき、それはAIでも同様だ。企業内に限った利用であればリスクは限定的だが、「顧客や社会のために利用すれば、リスクは人類の問題に発展しかねません」（礒田氏）。

　その点から、EUではEU域内でのAI技術の提供や使用を規制する法律「AI Act」が2024年8月から施行。AIの倫理・品質管理における企業が果たすべき社会的責任の形の1つが示された。

「AI ActはGDPRと同様、EU市場にサービスやシステムを提供するEU域外の企業にも適用されます。日本も当然、無関係でいることはできず、今から対応を始めても決して早くはありません」（礒田氏）

　対応に向け、公平性や説明可能性、透明性などの「責任あるAI原則」の実務への落とし込みが必要とされ、初めての経験だけに少なからぬ苦労も予想される。その中で礒田氏が最初に取り組むことを推奨するのがAIガバナンスの確立だ。


「まずは組織体制を構築し、リスク・アセスメントによりAIに関するリスクを評価します。その後は継続的なリスク軽減策に取り組むとともに、ポリシーなどを策定し、トレーニングも実施します。当初はとまどうことも多いでしょうが、これらに取り組んでいけば、自社のAI利用について説明できるようになります。セキュリティリーダーとしてリーダーシップを発揮し、取り組みをけん引してください」（礒田氏）
本記事は2024年7月24-26日に開催された「ガートナー セキュリティ ＆ リスク・マネジメント サミット」の講演内容をもとに再構成したものです。
執筆：フリーライター 岡崎 勝己",[],[]
